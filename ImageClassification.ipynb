{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Belajar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asetya/BigData/blob/master/ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oftw1TaJspp"
      },
      "source": [
        "praktikum mesin learning untuk image recognition "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DjUgIelJzqq"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import mahotas\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W1-CRTtitjG"
      },
      "source": [
        "jika ada pesan error ngga ditemukan mahotas jalankan kode instalasi mahotas di bahan ini (note - buang tanda #) \n",
        "\n",
        "\n",
        "```\n",
        "!pip install mahotas\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwSRskUaaQQD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3181425c-f122-4547-c5cc-398f793921d2"
      },
      "source": [
        "#!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/1f/01d805bc3588da8343373c279702d0fca4dc55f631873d9f2e159f9287ac/mahotas-1.4.10-cp36-cp36m-manylinux2010_x86_64.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mahotas) (1.18.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-YVDxrdX7RQ"
      },
      "source": [
        "Konek ke googl drive kita\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWSJpsyKqHjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d87650c0-25f1-464b-eb97-b9f1fb78dae3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY8RMcZOLDUQ"
      },
      "source": [
        "Copy Dataset dari lokal ke server google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M85MPr4h15ho"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/BIMBINGAN/Naskah_Thesis/MuhammadNafiuddin/imageclassification/dataset1.zip /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4teLdOLcOhA"
      },
      "source": [
        "**unzip dataset di server google colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWcdT3pmb_PO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cbc85cf-508f-4c02-8889-e0114eae7439"
      },
      "source": [
        "!unzip /content/dataset1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/dataset1.zip\n",
            "  inflating: dataset2/desktop.ini    \n",
            "   creating: dataset2/test/\n",
            "  inflating: dataset2/test/1.jpg     \n",
            "  inflating: dataset2/test/2.jpg     \n",
            "  inflating: dataset2/test/3.jpg     \n",
            "  inflating: dataset2/test/76.jpg    \n",
            "  inflating: dataset2/test/77.jpg    \n",
            "  inflating: dataset2/test/78.jpg    \n",
            "  inflating: dataset2/test/79.jpg    \n",
            "  inflating: dataset2/test/80.jpg    \n",
            "  inflating: dataset2/test/desktop.ini  \n",
            "   creating: dataset2/train/\n",
            "   creating: dataset2/train/bluebell/\n",
            "  inflating: dataset2/train/bluebell/1.jpg  \n",
            "  inflating: dataset2/train/bluebell/10.jpg  \n",
            "  inflating: dataset2/train/bluebell/11.jpg  \n",
            "  inflating: dataset2/train/bluebell/12.jpg  \n",
            "  inflating: dataset2/train/bluebell/13.jpg  \n",
            "  inflating: dataset2/train/bluebell/14.jpg  \n",
            "  inflating: dataset2/train/bluebell/15.jpg  \n",
            "  inflating: dataset2/train/bluebell/16.jpg  \n",
            "  inflating: dataset2/train/bluebell/17.jpg  \n",
            "  inflating: dataset2/train/bluebell/18.jpg  \n",
            "  inflating: dataset2/train/bluebell/19.jpg  \n",
            "  inflating: dataset2/train/bluebell/2.jpg  \n",
            "  inflating: dataset2/train/bluebell/20.jpg  \n",
            "  inflating: dataset2/train/bluebell/21.jpg  \n",
            "  inflating: dataset2/train/bluebell/22.jpg  \n",
            "  inflating: dataset2/train/bluebell/23.jpg  \n",
            "  inflating: dataset2/train/bluebell/24.jpg  \n",
            "  inflating: dataset2/train/bluebell/25.jpg  \n",
            "  inflating: dataset2/train/bluebell/26.jpg  \n",
            "  inflating: dataset2/train/bluebell/27.jpg  \n",
            "  inflating: dataset2/train/bluebell/28.jpg  \n",
            "  inflating: dataset2/train/bluebell/29.jpg  \n",
            "  inflating: dataset2/train/bluebell/3.jpg  \n",
            "  inflating: dataset2/train/bluebell/30.jpg  \n",
            "  inflating: dataset2/train/bluebell/31.jpg  \n",
            "  inflating: dataset2/train/bluebell/32.jpg  \n",
            "  inflating: dataset2/train/bluebell/33.jpg  \n",
            "  inflating: dataset2/train/bluebell/34.jpg  \n",
            "  inflating: dataset2/train/bluebell/35.jpg  \n",
            "  inflating: dataset2/train/bluebell/36.jpg  \n",
            "  inflating: dataset2/train/bluebell/37.jpg  \n",
            "  inflating: dataset2/train/bluebell/38.jpg  \n",
            "  inflating: dataset2/train/bluebell/39.jpg  \n",
            "  inflating: dataset2/train/bluebell/4.jpg  \n",
            "  inflating: dataset2/train/bluebell/40.jpg  \n",
            "  inflating: dataset2/train/bluebell/41.jpg  \n",
            "  inflating: dataset2/train/bluebell/42.jpg  \n",
            "  inflating: dataset2/train/bluebell/43.jpg  \n",
            "  inflating: dataset2/train/bluebell/44.jpg  \n",
            "  inflating: dataset2/train/bluebell/45.jpg  \n",
            "  inflating: dataset2/train/bluebell/46.jpg  \n",
            "  inflating: dataset2/train/bluebell/47.jpg  \n",
            "  inflating: dataset2/train/bluebell/48.jpg  \n",
            "  inflating: dataset2/train/bluebell/49.jpg  \n",
            "  inflating: dataset2/train/bluebell/5.jpg  \n",
            "  inflating: dataset2/train/bluebell/50.jpg  \n",
            "  inflating: dataset2/train/bluebell/51.jpg  \n",
            "  inflating: dataset2/train/bluebell/52.jpg  \n",
            "  inflating: dataset2/train/bluebell/53.jpg  \n",
            "  inflating: dataset2/train/bluebell/54.jpg  \n",
            "  inflating: dataset2/train/bluebell/55.jpg  \n",
            "  inflating: dataset2/train/bluebell/56.jpg  \n",
            "  inflating: dataset2/train/bluebell/57.jpg  \n",
            "  inflating: dataset2/train/bluebell/58.jpg  \n",
            "  inflating: dataset2/train/bluebell/59.jpg  \n",
            "  inflating: dataset2/train/bluebell/6.jpg  \n",
            "  inflating: dataset2/train/bluebell/60.jpg  \n",
            "  inflating: dataset2/train/bluebell/61.jpg  \n",
            "  inflating: dataset2/train/bluebell/62.jpg  \n",
            "  inflating: dataset2/train/bluebell/63.jpg  \n",
            "  inflating: dataset2/train/bluebell/64.jpg  \n",
            "  inflating: dataset2/train/bluebell/65.jpg  \n",
            "  inflating: dataset2/train/bluebell/66.jpg  \n",
            "  inflating: dataset2/train/bluebell/67.jpg  \n",
            "  inflating: dataset2/train/bluebell/68.jpg  \n",
            "  inflating: dataset2/train/bluebell/69.jpg  \n",
            "  inflating: dataset2/train/bluebell/7.jpg  \n",
            "  inflating: dataset2/train/bluebell/70.jpg  \n",
            "  inflating: dataset2/train/bluebell/71.jpg  \n",
            "  inflating: dataset2/train/bluebell/72.jpg  \n",
            "  inflating: dataset2/train/bluebell/73.jpg  \n",
            "  inflating: dataset2/train/bluebell/74.jpg  \n",
            "  inflating: dataset2/train/bluebell/75.jpg  \n",
            "  inflating: dataset2/train/bluebell/76.jpg  \n",
            "  inflating: dataset2/train/bluebell/77.jpg  \n",
            "  inflating: dataset2/train/bluebell/78.jpg  \n",
            "  inflating: dataset2/train/bluebell/79.jpg  \n",
            "  inflating: dataset2/train/bluebell/8.jpg  \n",
            "  inflating: dataset2/train/bluebell/80.jpg  \n",
            "  inflating: dataset2/train/bluebell/9.jpg  \n",
            "  inflating: dataset2/train/bluebell/desktop.ini  \n",
            "   creating: dataset2/train/buttercup/\n",
            "  inflating: dataset2/train/buttercup/1.jpg  \n",
            "  inflating: dataset2/train/buttercup/10.jpg  \n",
            "  inflating: dataset2/train/buttercup/11.jpg  \n",
            "  inflating: dataset2/train/buttercup/12.jpg  \n",
            "  inflating: dataset2/train/buttercup/13.jpg  \n",
            "  inflating: dataset2/train/buttercup/14.jpg  \n",
            "  inflating: dataset2/train/buttercup/15.jpg  \n",
            "  inflating: dataset2/train/buttercup/16.jpg  \n",
            "  inflating: dataset2/train/buttercup/17.jpg  \n",
            "  inflating: dataset2/train/buttercup/18.jpg  \n",
            "  inflating: dataset2/train/buttercup/19.jpg  \n",
            "  inflating: dataset2/train/buttercup/2.jpg  \n",
            "  inflating: dataset2/train/buttercup/20.jpg  \n",
            "  inflating: dataset2/train/buttercup/21.jpg  \n",
            "  inflating: dataset2/train/buttercup/22.jpg  \n",
            "  inflating: dataset2/train/buttercup/23.jpg  \n",
            "  inflating: dataset2/train/buttercup/24.jpg  \n",
            "  inflating: dataset2/train/buttercup/25.jpg  \n",
            "  inflating: dataset2/train/buttercup/26.jpg  \n",
            "  inflating: dataset2/train/buttercup/27.jpg  \n",
            "  inflating: dataset2/train/buttercup/28.jpg  \n",
            "  inflating: dataset2/train/buttercup/29.jpg  \n",
            "  inflating: dataset2/train/buttercup/3.jpg  \n",
            "  inflating: dataset2/train/buttercup/30.jpg  \n",
            "  inflating: dataset2/train/buttercup/31.jpg  \n",
            "  inflating: dataset2/train/buttercup/32.jpg  \n",
            "  inflating: dataset2/train/buttercup/33.jpg  \n",
            "  inflating: dataset2/train/buttercup/34.jpg  \n",
            "  inflating: dataset2/train/buttercup/35.jpg  \n",
            "  inflating: dataset2/train/buttercup/36.jpg  \n",
            "  inflating: dataset2/train/buttercup/37.jpg  \n",
            "  inflating: dataset2/train/buttercup/38.jpg  \n",
            "  inflating: dataset2/train/buttercup/39.jpg  \n",
            "  inflating: dataset2/train/buttercup/4.jpg  \n",
            "  inflating: dataset2/train/buttercup/40.jpg  \n",
            "  inflating: dataset2/train/buttercup/41.jpg  \n",
            "  inflating: dataset2/train/buttercup/42.jpg  \n",
            "  inflating: dataset2/train/buttercup/43.jpg  \n",
            "  inflating: dataset2/train/buttercup/44.jpg  \n",
            "  inflating: dataset2/train/buttercup/45.jpg  \n",
            "  inflating: dataset2/train/buttercup/46.jpg  \n",
            "  inflating: dataset2/train/buttercup/47.jpg  \n",
            "  inflating: dataset2/train/buttercup/48.jpg  \n",
            "  inflating: dataset2/train/buttercup/49.jpg  \n",
            "  inflating: dataset2/train/buttercup/5.jpg  \n",
            "  inflating: dataset2/train/buttercup/50.jpg  \n",
            "  inflating: dataset2/train/buttercup/51.jpg  \n",
            "  inflating: dataset2/train/buttercup/52.jpg  \n",
            "  inflating: dataset2/train/buttercup/53.jpg  \n",
            "  inflating: dataset2/train/buttercup/54.jpg  \n",
            "  inflating: dataset2/train/buttercup/55.jpg  \n",
            "  inflating: dataset2/train/buttercup/56.jpg  \n",
            "  inflating: dataset2/train/buttercup/57.jpg  \n",
            "  inflating: dataset2/train/buttercup/58.jpg  \n",
            "  inflating: dataset2/train/buttercup/59.jpg  \n",
            "  inflating: dataset2/train/buttercup/6.jpg  \n",
            "  inflating: dataset2/train/buttercup/60.jpg  \n",
            "  inflating: dataset2/train/buttercup/61.jpg  \n",
            "  inflating: dataset2/train/buttercup/62.jpg  \n",
            "  inflating: dataset2/train/buttercup/63.jpg  \n",
            "  inflating: dataset2/train/buttercup/64.jpg  \n",
            "  inflating: dataset2/train/buttercup/65.jpg  \n",
            "  inflating: dataset2/train/buttercup/66.jpg  \n",
            "  inflating: dataset2/train/buttercup/67.jpg  \n",
            "  inflating: dataset2/train/buttercup/68.jpg  \n",
            "  inflating: dataset2/train/buttercup/69.jpg  \n",
            "  inflating: dataset2/train/buttercup/7.jpg  \n",
            "  inflating: dataset2/train/buttercup/70.jpg  \n",
            "  inflating: dataset2/train/buttercup/71.jpg  \n",
            "  inflating: dataset2/train/buttercup/72.jpg  \n",
            "  inflating: dataset2/train/buttercup/73.jpg  \n",
            "  inflating: dataset2/train/buttercup/74.jpg  \n",
            "  inflating: dataset2/train/buttercup/75.jpg  \n",
            "  inflating: dataset2/train/buttercup/76.jpg  \n",
            "  inflating: dataset2/train/buttercup/77.jpg  \n",
            "  inflating: dataset2/train/buttercup/78.jpg  \n",
            "  inflating: dataset2/train/buttercup/79.jpg  \n",
            "  inflating: dataset2/train/buttercup/8.jpg  \n",
            "  inflating: dataset2/train/buttercup/80.jpg  \n",
            "  inflating: dataset2/train/buttercup/9.jpg  \n",
            "  inflating: dataset2/train/buttercup/desktop.ini  \n",
            "   creating: dataset2/train/coltsfoot/\n",
            "  inflating: dataset2/train/coltsfoot/1.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/10.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/11.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/12.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/13.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/14.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/15.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/16.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/17.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/18.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/19.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/2.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/20.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/21.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/22.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/23.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/24.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/25.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/26.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/27.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/28.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/29.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/3.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/30.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/31.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/32.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/33.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/34.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/35.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/36.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/37.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/38.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/39.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/4.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/40.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/41.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/42.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/43.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/44.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/45.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/46.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/47.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/48.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/49.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/5.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/50.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/51.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/52.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/53.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/54.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/55.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/56.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/57.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/58.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/59.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/6.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/60.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/61.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/62.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/63.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/64.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/65.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/66.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/67.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/68.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/69.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/7.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/70.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/71.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/72.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/73.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/74.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/75.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/76.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/77.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/78.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/79.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/8.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/80.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/9.jpg  \n",
            "  inflating: dataset2/train/coltsfoot/desktop.ini  \n",
            "  inflating: dataset2/train/desktop.ini  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP5RJSBDLGxZ"
      },
      "source": [
        "images_per_class = 80\n",
        "fixed_size       = tuple((500, 500))\n",
        "train_path       = \"dataset2/train\"\n",
        "h5_data          = 'output/data.h5'\n",
        "h5_labels        = 'output/labels.h5'\n",
        "bins             = 8\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goED1RLCLXLE"
      },
      "source": [
        "feture descriptor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2CXhkB9Laug"
      },
      "source": [
        "def fd_hu_moments(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
        "    return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw2dDR33Lg05"
      },
      "source": [
        "panggil feature detector hu moment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVKVoehULmtx"
      },
      "source": [
        "#!mkdir output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbOT31hXeOi7"
      },
      "source": [
        "**fungsi untuk mencari haralick feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "588GF0-NdkmV"
      },
      "source": [
        "def fd_haralick(image):\n",
        "    # convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # compute the haralick texture feature vector\n",
        "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
        "    # return the result\n",
        "    return haralick\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZBmy1IeZMK"
      },
      "source": [
        "**Fungsi untuk mencari histofram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJVySuddeNlL"
      },
      "source": [
        "def fd_histogram(image, mask=None):\n",
        "    # convert the image to HSV color-space\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    # compute the color histogram\n",
        "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
        "    # normalize the histogram\n",
        "    cv2.normalize(hist, hist)\n",
        "    # return the histogram\n",
        "    return hist.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5x_3OcReiyx"
      },
      "source": [
        "**MNgambil label training **\n",
        "Melakukan Training \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEvi8Xw2e1Fa"
      },
      "source": [
        "train_labels = os.listdir(train_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PaAQcDVe7p1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec23eb76-9627-49c4-cec1-ad56d778ea0a"
      },
      "source": [
        "print(train_labels)\n",
        "# sort the training labels\n",
        "train_labels.sort()\n",
        "print(train_labels)\n",
        "\n",
        "# empty lists to hold feature vectors and labels\n",
        "global_features = []\n",
        "labels          = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['coltsfoot', 'desktop.ini', 'bluebell', 'buttercup']\n",
            "['bluebell', 'buttercup', 'coltsfoot', 'desktop.ini']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPEqV2xgIGFM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "9d913b90-cf1a-46c0-cbdc-179a6db27fec"
      },
      "source": [
        "for training_name in train_labels:\n",
        "  dir = os.path.join(train_path, training_name)\n",
        "  if os.path.isdir(dir):\n",
        "    print(dir)\n",
        "    print(\" adalah folder\")\n",
        "    current_label = training_name\n",
        "    for x in range(1,images_per_class+1):\n",
        "          # get the image file name\n",
        "          file = dir + \"/\" + str(x) + \".jpg\"\n",
        "\n",
        "          # read the image and resize it to a fixed-size\n",
        "          image = cv2.imread(file)\n",
        "          image = cv2.resize(image, fixed_size)\n",
        "\n",
        "          ####################################\n",
        "          # Global Feature extraction\n",
        "          ####################################\n",
        "          fv_hu_moments = fd_hu_moments(image)\n",
        "          fv_haralick   = fd_haralick(image)\n",
        "          fv_histogram  = fd_histogram(image)\n",
        "\n",
        "          ###################################\n",
        "          # Concatenate global features\n",
        "          ###################################\n",
        "          global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
        "\n",
        "          # update the list of labels and feature vectors\n",
        "          labels.append(current_label)\n",
        "          global_features.append(global_feature)\n",
        "   \n",
        "    print(\"[STATUS] processed folder: {}\".format(current_label))\n",
        " \n",
        "# get the overall feature vector size\n",
        "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
        "\n",
        "# get the overall training label size\n",
        "print(\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
        "\n",
        "# encode the target labels\n",
        "targetNames = np.unique(labels)\n",
        "le          = LabelEncoder()\n",
        "target      = le.fit_transform(labels)\n",
        "print(\"[STATUS] training labels encoded...\")\n",
        "\n",
        "# scale features in the range (0-1)\n",
        "scaler            = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_features = scaler.fit_transform(global_features)\n",
        "print(\"[STATUS] feature vector normalized...\")\n",
        "\n",
        "print(\"[STATUS] target labels: {}\".format(target))\n",
        "print(\"[STATUS] target labels shape: {}\".format(target.shape))\n",
        "\n",
        "# save the feature vector using HDF5\n",
        "h5f_data = h5py.File(h5_data, 'w')\n",
        "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
        "\n",
        "h5f_label = h5py.File(h5_labels, 'w')\n",
        "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
        "\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "\n",
        "print(\"[STATUS] end of training..\")    \n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset2/train/bluebell\n",
            " adalah folder\n",
            "[STATUS] processed folder: bluebell\n",
            "dataset2/train/buttercup\n",
            " adalah folder\n",
            "[STATUS] processed folder: buttercup\n",
            "dataset2/train/coltsfoot\n",
            " adalah folder\n",
            "[STATUS] processed folder: coltsfoot\n",
            "[STATUS] feature vector size (480, 532)\n",
            "[STATUS] training Labels (480,)\n",
            "[STATUS] training labels encoded...\n",
            "[STATUS] feature vector normalized...\n",
            "[STATUS] target labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[STATUS] target labels shape: (480,)\n",
            "[STATUS] end of training..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8L2IyjfjZE"
      },
      "source": [
        "**Loop Training untuk seluruh sub folder **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIrP1z4imm8P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uu2WkMQtvYe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1238bb3b-6e4b-40ba-e3d3-2d738630bf11"
      },
      "source": [
        "\n",
        "import glob\n",
        "import warnings\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.externals import joblib\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}