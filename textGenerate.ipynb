{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textGenerate.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzu3lgsWo4V0eH4q5O+3eQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asetya/BigData/blob/master/textGenerate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Data Science Libraries\n",
        "import pickle\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "# Neural Net Preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Neural Net Layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Neural Net Training\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from pickle import load"
      ],
      "metadata": {
        "id": "Ygd5AXfU69KX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "bcPtn3O75iBC",
        "outputId": "d294c943-0290-4d50-dd9c-2347e7dc4956"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ea29e67-c0ad-4809-980f-42485f53c0e0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2ea29e67-c0ad-4809-980f-42485f53c0e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRdUlAsr1ypP",
        "outputId": "534f57d0-01c7-4e3c-f9de-ee885e2ceaa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences:  7900\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Import the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "# Selecting Edgar Allen Poe as author style to emulate\n",
        "author = train_df[train_df['author'] == 'EAP'][\"text\"]\n",
        "print('Number of training sentences: ',author.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 50000 # Max size of the dictionary\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(author.values)\n",
        "sequences = tokenizer.texts_to_sequences(author.values)\n",
        "\n",
        "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
        "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
        "text = [item for sublist in sequences for item in sublist]\n",
        "vocab_size = len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "QJWYtj3P6QtL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training on 19 words to predict the 20th\n",
        "sentence_len = 20\n",
        "pred_len = 1\n",
        "train_len = sentence_len - pred_len\n",
        "seq = []\n",
        "# Sliding window to generate train data\n",
        "for i in range(len(text)-sentence_len):\n",
        "    seq.append(text[i:i+sentence_len])\n",
        "# Reverse dictionary to decode tokenized sequences back to words\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
        "trainX = []\n",
        "trainy = []\n",
        "for i in seq:\n",
        "    trainX.append(i[:train_len])\n",
        "    trainy.append(i[-1])"
      ],
      "metadata": {
        "id": "eUG0Xyru7H6v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model_2 = Sequential([\n",
        "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Train model with checkpoints\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "filepath = \"./model_2_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "history = model_2.fit(np.asarray(trainX),\n",
        "         pd.get_dummies(np.asarray(trainy)),\n",
        "         epochs = 30,\n",
        "         batch_size = 128,\n",
        "         callbacks = callbacks_list,\n",
        "         verbose = 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEg-H9cy7JqJ",
        "outputId": "a785f20a-026b-4a83-97c9-050304b8a4a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 6.7996 - accuracy: 0.0873\n",
            "Epoch 1: loss improved from inf to 6.79965, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 28s 13ms/step - loss: 6.7996 - accuracy: 0.0873\n",
            "Epoch 2/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 6.3382 - accuracy: 0.1086\n",
            "Epoch 2: loss improved from 6.79965 to 6.33795, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 6.3379 - accuracy: 0.1086\n",
            "Epoch 3/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 6.1272 - accuracy: 0.1227\n",
            "Epoch 3: loss improved from 6.33795 to 6.12715, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 6.1272 - accuracy: 0.1227\n",
            "Epoch 4/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 5.9725 - accuracy: 0.1302\n",
            "Epoch 4: loss improved from 6.12715 to 5.97252, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.9725 - accuracy: 0.1302\n",
            "Epoch 5/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 5.8414 - accuracy: 0.1360\n",
            "Epoch 5: loss improved from 5.97252 to 5.84141, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.8414 - accuracy: 0.1360\n",
            "Epoch 6/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 5.7203 - accuracy: 0.1402\n",
            "Epoch 6: loss improved from 5.84141 to 5.72031, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.7203 - accuracy: 0.1402\n",
            "Epoch 7/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 5.6116 - accuracy: 0.1436\n",
            "Epoch 7: loss improved from 5.72031 to 5.61161, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.6116 - accuracy: 0.1436\n",
            "Epoch 8/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 5.5118 - accuracy: 0.1469\n",
            "Epoch 8: loss improved from 5.61161 to 5.51175, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.5118 - accuracy: 0.1469\n",
            "Epoch 9/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 5.4191 - accuracy: 0.1492\n",
            "Epoch 9: loss improved from 5.51175 to 5.41872, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.4187 - accuracy: 0.1492\n",
            "Epoch 10/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 5.3333 - accuracy: 0.1519\n",
            "Epoch 10: loss improved from 5.41872 to 5.33333, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.3333 - accuracy: 0.1519\n",
            "Epoch 11/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 5.2520 - accuracy: 0.1549\n",
            "Epoch 11: loss improved from 5.33333 to 5.25214, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.2521 - accuracy: 0.1549\n",
            "Epoch 12/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 5.1763 - accuracy: 0.1572\n",
            "Epoch 12: loss improved from 5.25214 to 5.17627, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.1763 - accuracy: 0.1572\n",
            "Epoch 13/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 5.1027 - accuracy: 0.1597\n",
            "Epoch 13: loss improved from 5.17627 to 5.10314, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.1031 - accuracy: 0.1596\n",
            "Epoch 14/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 5.0353 - accuracy: 0.1628\n",
            "Epoch 14: loss improved from 5.10314 to 5.03524, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 5.0352 - accuracy: 0.1628\n",
            "Epoch 15/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 4.9705 - accuracy: 0.1648\n",
            "Epoch 15: loss improved from 5.03524 to 4.97070, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.9707 - accuracy: 0.1649\n",
            "Epoch 16/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 4.9112 - accuracy: 0.1674\n",
            "Epoch 16: loss improved from 4.97070 to 4.91140, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.9114 - accuracy: 0.1674\n",
            "Epoch 17/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 4.8543 - accuracy: 0.1695\n",
            "Epoch 17: loss improved from 4.91140 to 4.85422, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.8542 - accuracy: 0.1695\n",
            "Epoch 18/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 4.8003 - accuracy: 0.1718\n",
            "Epoch 18: loss improved from 4.85422 to 4.80025, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.8002 - accuracy: 0.1718\n",
            "Epoch 19/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 4.7543 - accuracy: 0.1738\n",
            "Epoch 19: loss improved from 4.80025 to 4.75432, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.7543 - accuracy: 0.1738\n",
            "Epoch 20/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 4.7078 - accuracy: 0.1764\n",
            "Epoch 20: loss improved from 4.75432 to 4.70831, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.7083 - accuracy: 0.1764\n",
            "Epoch 21/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 4.6637 - accuracy: 0.1782\n",
            "Epoch 21: loss improved from 4.70831 to 4.66333, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.6633 - accuracy: 0.1783\n",
            "Epoch 22/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 4.6389 - accuracy: 0.1807\n",
            "Epoch 22: loss improved from 4.66333 to 4.63925, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.6393 - accuracy: 0.1806\n",
            "Epoch 23/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 4.5906 - accuracy: 0.1825\n",
            "Epoch 23: loss improved from 4.63925 to 4.59060, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.5906 - accuracy: 0.1825\n",
            "Epoch 24/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 4.5495 - accuracy: 0.1860\n",
            "Epoch 24: loss improved from 4.59060 to 4.54947, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.5495 - accuracy: 0.1860\n",
            "Epoch 25/30\n",
            "1569/1571 [============================>.] - ETA: 0s - loss: 4.5126 - accuracy: 0.1886\n",
            "Epoch 25: loss improved from 4.54947 to 4.51238, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.5124 - accuracy: 0.1886\n",
            "Epoch 26/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 4.4803 - accuracy: 0.1899\n",
            "Epoch 26: loss improved from 4.51238 to 4.48021, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.4802 - accuracy: 0.1900\n",
            "Epoch 27/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 4.4434 - accuracy: 0.1931\n",
            "Epoch 27: loss improved from 4.48021 to 4.44336, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.4434 - accuracy: 0.1931\n",
            "Epoch 28/30\n",
            "1568/1571 [============================>.] - ETA: 0s - loss: 4.4116 - accuracy: 0.1955\n",
            "Epoch 28: loss improved from 4.44336 to 4.41149, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.4115 - accuracy: 0.1955\n",
            "Epoch 29/30\n",
            "1570/1571 [============================>.] - ETA: 0s - loss: 4.3780 - accuracy: 0.1979\n",
            "Epoch 29: loss improved from 4.41149 to 4.37809, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.3781 - accuracy: 0.1979\n",
            "Epoch 30/30\n",
            "1571/1571 [==============================] - ETA: 0s - loss: 4.3459 - accuracy: 0.2001\n",
            "Epoch 30: loss improved from 4.37809 to 4.34588, saving model to ./model_2_weights.hdf5\n",
            "1571/1571 [==============================] - 20s 13ms/step - loss: 4.3459 - accuracy: 0.2001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(model,seq,max_len = 20):\n",
        "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
        "    reaches max_len'''\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
        "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
        "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
        "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "        tokenized_sent[0].append(op.argmax()+1)\n",
        "        \n",
        "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
      ],
      "metadata": {
        "id": "3IDlll9I_xYc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen(model_2,\"First of all I dismembered the corpse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FUI-gzfe_5LL",
        "outputId": "8d24e484-6062-4f34-8eea-a63d7602baf9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'first of all i dismembered the corpse and the most lady is the most lasting of the house and the most lady is the most lasting of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}