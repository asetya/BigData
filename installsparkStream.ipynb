{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "installsparkStream.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asetya/BigData/blob/master/installsparkStream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh2Aze97vwoC"
      },
      "source": [
        "**PERSIAPAN**\n",
        "\n",
        "1. Install Java open Jdk "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoeBBk7Lvnfp"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7CigBRBv5l5"
      },
      "source": [
        "2. Download spark \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zw5UGhov9Ap"
      },
      "source": [
        "# Install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psJa7fVBSDRI"
      },
      "source": [
        "3. Unzip spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AlBtPCWwGSm"
      },
      "source": [
        "# Unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTGpkfFwdWX"
      },
      "source": [
        "4. Setting Java Home dan Spark Home"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsDdeYAnwevw"
      },
      "source": [
        "# Set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNPKO8b9NmyV"
      },
      "source": [
        "6. Install findspark "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF8lxHU0wlVn"
      },
      "source": [
        "# Install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmXB2_thNqfp"
      },
      "source": [
        "7. Inisialisasi Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuUGOZYfJVxM"
      },
      "source": [
        "import findspark\n",
        "findspark.init() "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DjWrw6gNvO6"
      },
      "source": [
        "8. Buat Fungsi khusus untuk parsing data dari sensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Mv3ERoKZKb"
      },
      "source": [
        "# Parse a line of weather station data, returning the average wind direction measurement \n",
        "#\n",
        "import re\n",
        "def parse(line):\n",
        "    match = re.search(\"Dm=(\\d+)\", line)\n",
        "    if match:\n",
        "        val = match.group(1)\n",
        "        return [int(val)]\n",
        "    return []"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr6wPCf7N02o"
      },
      "source": [
        "**RUN THE STREAMING**\n",
        "\n",
        "1. Import and create streaming context. Next, we will import and create a new instance of Spark's StreamingContext:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGbuLjZhxMAE"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "#create spark context\n",
        "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
        "#membuat streaming context dengan batch interval = 1 detik \n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gI04jkEqPiY"
      },
      "source": [
        "2. Membuat koneksi stream dengan data weather pada alamat rtd.hpwren.ucsd.edu port 12028"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4vCLBsQp5cG"
      },
      "source": [
        "#membaca baris demi baris pada rtd.hpwren.ucsd.edu socket 12028 \n",
        "lines = ssc.socketTextStream(\"rtd.hpwren.ucsd.edu\", 12020)\n",
        "# Read measurement. Next, let's read the average wind speed from each line and store it in a new DStream vals:\n",
        "vals = lines.flatMap(parse)\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MP8wuwhrAXw"
      },
      "source": [
        "Membuat sliding window  data 10 detik dan bergeser setiap 5 detik\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lho6doPMq2pd"
      },
      "source": [
        "window = vals.window(10,5)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCFSEWjprJNM"
      },
      "source": [
        "Mebuat fungsi untuk melakukan analisis - misalnya kita bikin fungsi untuk mencari nilai minimium dan maksimum dalam sebuiahg winwdws "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGY0VxSPrZK8"
      },
      "source": [
        "def stats(rdd): \n",
        "  print(rdd.collect())\n",
        "  if rdd.count()>0:\n",
        "    print (\"max ={},min={}\".format(rdd.max(), rdd.min()))\n",
        "  \n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kBnFMYr3Pd"
      },
      "source": [
        "memanggil fungsi stats() untuk setiap RDD [ada sliding window kita"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID79ljs-r_-Q"
      },
      "source": [
        "window.foreachRDD(lambda rdd:stats(rdd))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbrxj3bjOHEa"
      },
      "source": [
        "\n",
        "\n",
        "2. menjalankan streaming "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDMspstOMKQJ"
      },
      "source": [
        "#@title\n",
        "ssc.start()"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg7QePpXOJe4"
      },
      "source": [
        "3. Mengakhiri streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVVKhr7eMO2Z",
        "outputId": "1a8db711-4b8d-4769-f066-eb676381899f"
      },
      "source": [
        "ssc.stop()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[104, 104, 104, 104]\n",
            "max =104,min=104\n",
            "[15, 15, 15, 15, 15]\n",
            "max =15,min=15\n",
            "[104, 104, 104, 104, 104, 103, 103, 103, 104]\n",
            "max =104,min=103\n",
            "[15, 15, 15, 15, 15, 15, 15, 15, 15, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}